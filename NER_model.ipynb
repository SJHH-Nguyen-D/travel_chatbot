{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5IlAyS8PPVov",
        "kdNT9auiRoKO",
        "DvdBuHQtKXTG"
      ],
      "mount_file_id": "1OYlG4YphGkIwLI0Nnqv9HIGc-UdKa7-v",
      "authorship_tag": "ABX9TyPsmY9MM715egoRIr3y74Af"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttHOzxT3LRCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! sudo apt install openjdk-8-jdk\n",
        "# ! sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java \n",
        "# ! pip install language-check -qq\n",
        "# ! pip install pycontractions -qq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYilNPsPjxYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install chatterbot \n",
        "# ! pip install chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j15dAlF_sKtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install --upgrade chatterbot \n",
        "# ! pip install --upgrade chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krjAXNNE_pGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import statements\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint as pp\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import re\n",
        "from timeit import default_timer\n",
        "\n",
        "# Preprocessing\n",
        "# from pycontractions import Contractions\n",
        "\n",
        "# Tokenization imports\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Puncuation & lower case\n",
        "import string #punctuation removal\n",
        "\n",
        "# Stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Lemmatizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# POS tagging\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# NER\n",
        "import nltk, nltk.tag, nltk.chunk \n",
        "import spacy\n",
        "import pprint as pprint\n",
        "from gensim.summarization import summarize \n",
        "from collections import Counter \n",
        "import en_core_web_sm # CNN gets loaded in, sees what words depends on each other, POS tagging, entity recognition \n",
        "from spacy import displacy # Visualize NER\n",
        "\n",
        "# Chatterbot\n",
        "# from chatterbot import ChatBot\n",
        "# from chatterbot.trainers import ListTrainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAAajV7zKU_",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju63vJs-FRiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is testing with part 1 - 10% of data\n",
        "with open('/content/drive/My Drive/contraction_data_parts/expand_convo_text_1.txt', 'r') as file:\n",
        "    convo_all = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pen9_RKDsJtO",
        "colab_type": "code",
        "outputId": "449976c5-df7f-469b-ff27-54c6122d1522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Let's see the text data, seems messy \n",
        "# When we extracted conversations from the frames dataset, we split every statement with a new line and every conversations with *\n",
        "# Through the use of pycontractions, every new line (\\n) was added with an extra \\\n",
        "pp.pprint(convo_all[0:1000]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('[\"I would like to book a trip to Atlantis from Caprica on Saturday, August '\n",
            " '13, 2016 for 8 adults. I have a tight budget of 1700.\\\\nHi...I checked a few '\n",
            " 'options for you, and unfortunately, we do not currently have any trips that '\n",
            " 'meet this criteria.  Would you like to book an alternate travel '\n",
            " 'option?\\\\nYes, how about going to Neverland from Caprica on August 13, 2016 '\n",
            " 'for 5 adults. For this trip, my budget would be 1900.\\\\nI checked the '\n",
            " 'availability for this date and there were no trips available.  Would you '\n",
            " 'like to select some alternate dates?\\\\nI have no flexibility for dates... '\n",
            " 'but I can leave from Atlantis rather than Caprica. How about that?\\\\nI '\n",
            " 'checked the availability for that date and there were no trips available.  '\n",
            " 'Would you like to select some alternate dates?\\\\nI suppose I will speak with '\n",
            " 'my husband to see if we can choose other dates, and then I will come back to '\n",
            " 'you.Thanks for your help\\\\n******************************************Hello, '\n",
            " 'I am looking to book a vacation from Gotham Ci')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOJOPGk3Xrs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits text data into separate lists for each conversation\n",
        "def text_to_convo(text):\n",
        "  list_convos = [convo.split('\\n') for convo in text.split('*') if text]         # Conversation delimited by *\n",
        "  list_convos = [convo for convo in list_convos if convo != ['']]                # Remove empty conversation\n",
        "  return list_convos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrriLBqZoT7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits all the statements from a conversation into their own string\n",
        "def convo_to_statement(list_convos):\n",
        "  sep_convos = []\n",
        "  for items in list_convos: \n",
        "    sep_convos.append([])                     # Creates list for each conversation\n",
        "    for item in items:\n",
        "      item = item.split('\\\\n')                # Splits conversations into statements \n",
        "      sep_convos[-1].append(item)             # Adds all statements to corresponding list\n",
        "  return sep_convos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg9ULJTA7WAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flattens the list as there is a list in another list\n",
        "# - Handles sentence tokenization, as all statements are separated into their own string in this list \n",
        "def flatten_list(conversation): \n",
        "  flat_list = []\n",
        "  for sublist in conversation:\n",
        "    for item in sublist:\n",
        "          flat_list.append(item)\n",
        "  return flat_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPj0T71h5Vl9",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xYbWKne5bnM",
        "colab_type": "code",
        "outputId": "b4d765c3-7afd-4d98-a3d9-af67e22fcfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt') # Punkt sentence tokenizer"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p3Imi6DLhFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits sentences into words \n",
        "def word_token(flat_list):\n",
        "  tokenized_words=[]\n",
        "  tokenized_words.extend(word for word in word_tokenize(str(flat_list))) # Extends list by appending elements from the iterable\n",
        "  return tokenized_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6F_mTQzFnSP",
        "colab_type": "text"
      },
      "source": [
        "## Punctuation & Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3PxB5H4Qzwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string.punctuation = string.punctuation + '’'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDUbmbvXus2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13d00c6e-0419-4949-c7a6-2f18a835e92f"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_CPUNqFTrOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# might reconsider as it removes : from time 2:03 -> 203\n",
        "# Removes punctuation from all strings \n",
        "# - Decided to use translate() to remove pesky characters that were attatched to the string\n",
        "def punc_removal(tokenized_words):\n",
        "  translator = str.maketrans('', '', string.punctuation)                        # Construct translator\n",
        "  no_punct = [word.translate(translator) for word in tokenized_words]           # Removes punctuation\n",
        "  no_punct = [word for word in no_punct if word != '']                          # Removes empty strings\n",
        "  return no_punct             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ofBua-nGuYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Makes every word lowercase \n",
        "def word_to_lowercase(word_list):\n",
        "  word_lower = [word.lower() for word in word_list]\n",
        "  return word_lower"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvNQtNPVG49I",
        "colab_type": "text"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgIaa9qiIuFP",
        "colab_type": "code",
        "outputId": "1ab8c5a7-418a-4332-ed7a-e3e429460c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJUMTOcTIxZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "# add_stop_words = ['would','could']\n",
        "# stop_words += add_stop_words\n",
        "# print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6gxeqbRIfQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stop_word_removal(word_list):\n",
        "  stop_words = stopwords.words('english')\n",
        "  filtered_word = []\n",
        "  for word in word_list:\n",
        "    if word not in stop_words:\n",
        "      filtered_word.append(word)\n",
        "  return filtered_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IlAyS8PPVov",
        "colab_type": "text"
      },
      "source": [
        "### Emoji removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GslwOGSgOgIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for word in filtered_word[:]:       #makes a copy of the list words and then iterates over that copy. Then, modifies the original list.\n",
        "#     if word.endswith('_face'): \n",
        "#         filtered_word.remove(word) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdNT9auiRoKO",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJet3LjsJAjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english', ignore_stopwords=True) # Already removed stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwauVi_ZYpDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_words = []\n",
        "\n",
        "for words in filtered_sent:\n",
        "  \n",
        "  stemmed_words.append(stemmer.stem(words)) \n",
        "\n",
        "  print('Words '+words+' - stemmer:'+stemmer.stem(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z20xkS5uSJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_words = []\n",
        "for word in stemmed_words:\n",
        "  if word not in stop_words:\n",
        "    filtered_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fE0Dx67u5Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpQCZqm1u8Rp",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatizing with appropriate POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnoQv-mzbfT",
        "colab_type": "code",
        "outputId": "6f977666-349b-4a9c-c8f9-03b6f49bafb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "nltk.download('wordnet')                        # Lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')     # POS tagging"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CV6dH8mA0zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maps POS tag to first character lemmatize() accepts\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    dict_tag = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return dict_tag.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXmXofCZA7a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize all words with the appropriate POS tag\n",
        "def word_lemmatization(filtered_word):\n",
        "  lem_words = []\n",
        "  lem = WordNetLemmatizer()\n",
        "  lem_words = [lem.lemmatize(word, get_wordnet_pos(word)) for word in filtered_word if word]\n",
        "  return lem_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFm2B4ef16DS",
        "colab_type": "text"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCMsG_0ps0Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pip install spacy -qq "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo4q3c3uY6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities\n",
        "def ner_model(sample_conversation):\n",
        "  nlp = en_core_web_sm.load\n",
        "  text = ' '.join(sample_conversation)\n",
        "  convo_nlp = nlp(text)\n",
        "  return convo_nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FMu-wPMat_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Renders a dependency parse tree or named entity visualization\n",
        "def ner_model_viz(document):\n",
        "  viz = displacy.render(document, jupyter=True, style='ent', page=True)\n",
        "  return viz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMMroeapaJJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "219d2a97-b5cd-43cf-88de-01753537c102"
      },
      "source": [
        "for entity in convo_nlp.ents:\n",
        "  print(entity.label_, ' | ', entity.text)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPE  |  chicago\n",
            "DATE  |  august 26th september 5th\n",
            "GPE  |  chicago\n",
            "CARDINAL  |  2300\n",
            "GPE  |  chicago\n",
            "CARDINAL  |  3\n",
            "DATE  |  4 day\n",
            "GPE  |  san diego\n",
            "CARDINAL  |  88610\n",
            "DATE  |  nice day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep8ng0TMq9YV",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZBl2u5A2VYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess frames data set using functions above \n",
        "def data_preprocess(text_data):\n",
        "  list_of_convo = text_to_convo(text_data) \n",
        "  list_of_statement = convo_to_statement(list_of_convo)\n",
        "  return list_of_statement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YRyaZ3D2bqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess a conversation using all functions above\n",
        "def convo_preprocess(conversation):\n",
        "  flat_convo = flatten_list(conversation)\n",
        "  list_of_word = word_token(flat_convo)\n",
        "  no_punct_word = punc_removal(list_of_word)\n",
        "  lower_word = word_to_lowercase(no_punct_word)\n",
        "  no_stop_word = stop_word_removal(lower_word)\n",
        "  lem_word = word_lemmatization(no_stop_word)\n",
        "  return lem_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWB3vDWsRt16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_convo = data_preprocess(convo_all)\n",
        "sample_convo_train = convo_preprocess(train_convo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4W9uv_5Tsmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_convo = all_convo[114]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCL78x5OUi6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_convo = all_convo[90]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvdBuHQtKXTG",
        "colab_type": "text"
      },
      "source": [
        "# Chatterbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ12G7U4ibb7",
        "colab_type": "code",
        "outputId": "d09e7bf9-c8ff-4301-fbbf-db344f6d6c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Create a new chat bot named Charlie\n",
        "chatbot = ChatBot('Charlie')\n",
        "trainer = ListTrainer(chatbot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50vn2vPxwteJ",
        "colab_type": "code",
        "outputId": "7eb330e6-1e08-4689-d80e-f380bb1d9b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer.train([\n",
        "    'How are you?',\n",
        "    'I am good.',\n",
        "    'That is good to hear.',\n",
        "    'Thank you',\n",
        "    'You are welcome.',\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOohofQ5sgJ",
        "colab_type": "code",
        "outputId": "672bae01-3b0c-45ea-c70c-c525344f6c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer.train(flat_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzLGP8kKsBmG",
        "colab_type": "code",
        "outputId": "83ff2809-129d-4816-c043-e549b2b259f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(chatbot.get_response('Hi'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To start, just give me some information on where you would like to travel, your budget, your point of departure, or any other travel info you can think of.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVG3GUofrJ24",
        "colab_type": "code",
        "outputId": "0683069c-d081-4b26-b7ae-18f900ee6e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('I would like to travel from Toronto to Japan'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Did you have any specific dates in mind?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTYd5-j7_C1",
        "colab_type": "code",
        "outputId": "f3529a0f-593c-4a84-f376-11f69f9c625c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('June to August'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How much would that be?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9-q63wu8JRS",
        "colab_type": "code",
        "outputId": "31619179-dc76-4848-a04d-d3ae9cb11280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('under 1200'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perfect. Thank you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhAhlBR68NOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}