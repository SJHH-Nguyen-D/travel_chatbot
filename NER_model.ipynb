{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5IlAyS8PPVov",
        "kdNT9auiRoKO",
        "DvdBuHQtKXTG"
      ],
      "mount_file_id": "1OYlG4YphGkIwLI0Nnqv9HIGc-UdKa7-v",
      "authorship_tag": "ABX9TyND2x6J4NfZIeqGXozmrx0W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttHOzxT3LRCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! sudo apt install openjdk-8-jdk\n",
        "# ! sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java \n",
        "# ! pip install language-check -qq\n",
        "# ! pip install pycontractions -qq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYilNPsPjxYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install chatterbot \n",
        "# ! pip install chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j15dAlF_sKtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install --upgrade chatterbot \n",
        "# ! pip install --upgrade chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krjAXNNE_pGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import statements\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint as pp\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import re\n",
        "from timeit import default_timer\n",
        "from  more_itertools import unique_everseen\n",
        "\n",
        "# Preprocessing\n",
        "# from pycontractions import Contractions\n",
        "\n",
        "# Tokenization imports\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Puncuation & lower case\n",
        "import string #punctuation removal\n",
        "\n",
        "# Stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Lemmatizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# POS tagging\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# NER\n",
        "import nltk, nltk.tag, nltk.chunk \n",
        "import spacy\n",
        "import pprint as pprint\n",
        "from gensim.summarization import summarize \n",
        "from collections import Counter \n",
        "import en_core_web_sm # CNN gets loaded in, sees what words depends on each other, POS tagging, entity recognition \n",
        "from spacy import displacy # Visualize NER\n",
        "\n",
        "# Chatterbot\n",
        "# from chatterbot import ChatBot\n",
        "# from chatterbot.trainers import ListTrainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAAajV7zKU_",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju63vJs-FRiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is testing with part 1 - 10% of data\n",
        "with open('/content/drive/My Drive/contraction_data_parts/expand_convo_text_1.txt', 'r') as file:\n",
        "    convo_all = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pen9_RKDsJtO",
        "colab_type": "code",
        "outputId": "449976c5-df7f-469b-ff27-54c6122d1522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Let's see the text data, seems messy \n",
        "# When we extracted conversations from the frames dataset, we split every statement with a new line and every conversations with *\n",
        "# Through the use of pycontractions, every new line (\\n) was added with an extra \\\n",
        "pp.pprint(convo_all[0:1000]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('[\"I would like to book a trip to Atlantis from Caprica on Saturday, August '\n",
            " '13, 2016 for 8 adults. I have a tight budget of 1700.\\\\nHi...I checked a few '\n",
            " 'options for you, and unfortunately, we do not currently have any trips that '\n",
            " 'meet this criteria.  Would you like to book an alternate travel '\n",
            " 'option?\\\\nYes, how about going to Neverland from Caprica on August 13, 2016 '\n",
            " 'for 5 adults. For this trip, my budget would be 1900.\\\\nI checked the '\n",
            " 'availability for this date and there were no trips available.  Would you '\n",
            " 'like to select some alternate dates?\\\\nI have no flexibility for dates... '\n",
            " 'but I can leave from Atlantis rather than Caprica. How about that?\\\\nI '\n",
            " 'checked the availability for that date and there were no trips available.  '\n",
            " 'Would you like to select some alternate dates?\\\\nI suppose I will speak with '\n",
            " 'my husband to see if we can choose other dates, and then I will come back to '\n",
            " 'you.Thanks for your help\\\\n******************************************Hello, '\n",
            " 'I am looking to book a vacation from Gotham Ci')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOJOPGk3Xrs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits text data into separate lists for each conversation\n",
        "def text_to_convo(text):\n",
        "  list_convos = [convo.split('\\n') for convo in text.split('*') if text]         # Conversation delimited by *\n",
        "  list_convos = [convo for convo in list_convos if convo != ['']]                # Remove empty conversation\n",
        "  return list_convos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrriLBqZoT7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits all the statements from a conversation into their own string\n",
        "def convo_to_statement(list_convos):\n",
        "  sep_convos = []\n",
        "  for items in list_convos: \n",
        "    sep_convos.append([])                     # Creates list for each conversation\n",
        "    for item in items:\n",
        "      item = item.split('\\\\n')                # Splits conversations into statements \n",
        "      sep_convos[-1].append(item)             # Adds all statements to corresponding list\n",
        "  return sep_convos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg9ULJTA7WAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flattens the list as there is a list in another list\n",
        "# - Handles sentence tokenization, as all statements are separated into their own string in this list \n",
        "def flatten_list(conversation): \n",
        "  flat_list = []\n",
        "  for sublist in conversation:\n",
        "    for item in sublist:\n",
        "          flat_list.append(item)\n",
        "  return flat_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPj0T71h5Vl9",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xYbWKne5bnM",
        "colab_type": "code",
        "outputId": "b4d765c3-7afd-4d98-a3d9-af67e22fcfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt') # Punkt sentence tokenizer"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p3Imi6DLhFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits sentences into words \n",
        "def word_token(flat_list):\n",
        "  tokenized_words=[]\n",
        "  tokenized_words.extend(word for word in word_tokenize(str(flat_list))) # Extends list by appending elements from the iterable\n",
        "  return tokenized_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6F_mTQzFnSP",
        "colab_type": "text"
      },
      "source": [
        "## Punctuation & Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDUbmbvXus2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string.punctuation = string.punctuation.replace('$','’')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_CPUNqFTrOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# might reconsider as it removes : from time 2:03 -> 203\n",
        "# Removes punctuation from all strings \n",
        "# - Decided to use translate() to remove pesky characters that were attatched to the string\n",
        "def punc_removal(tokenized_words):\n",
        "  translator = str.maketrans('', '', string.punctuation)                        # Construct translator\n",
        "  no_punct = [word.translate(translator) for word in tokenized_words]           # Removes punctuation\n",
        "  no_punct = [word for word in no_punct if word != '']                          # Removes empty strings\n",
        "  return no_punct             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ofBua-nGuYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Makes every word lowercase \n",
        "def word_to_lowercase(word_list):\n",
        "  word_lower = [word.lower() for word in word_list]\n",
        "  return word_lower"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvNQtNPVG49I",
        "colab_type": "text"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgIaa9qiIuFP",
        "colab_type": "code",
        "outputId": "1ab8c5a7-418a-4332-ed7a-e3e429460c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJUMTOcTIxZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "# add_stop_words = ['would','could']\n",
        "# stop_words += add_stop_words\n",
        "# print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6gxeqbRIfQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stop_word_removal(word_list):\n",
        "  stop_words = stopwords.words('english')\n",
        "  filtered_word = []\n",
        "  for word in word_list:\n",
        "    if word not in stop_words:\n",
        "      filtered_word.append(word)\n",
        "  return filtered_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IlAyS8PPVov",
        "colab_type": "text"
      },
      "source": [
        "### Emoji removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GslwOGSgOgIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for word in filtered_word[:]:       #makes a copy of the list words and then iterates over that copy. Then, modifies the original list.\n",
        "#     if word.endswith('_face'): \n",
        "#         filtered_word.remove(word) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdNT9auiRoKO",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJet3LjsJAjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english', ignore_stopwords=True) # Already removed stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwauVi_ZYpDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_words = []\n",
        "\n",
        "for words in filtered_sent:\n",
        "  \n",
        "  stemmed_words.append(stemmer.stem(words)) \n",
        "\n",
        "  print('Words '+words+' - stemmer:'+stemmer.stem(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z20xkS5uSJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_words = []\n",
        "for word in stemmed_words:\n",
        "  if word not in stop_words:\n",
        "    filtered_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fE0Dx67u5Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpQCZqm1u8Rp",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatizing with appropriate POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnoQv-mzbfT",
        "colab_type": "code",
        "outputId": "6f977666-349b-4a9c-c8f9-03b6f49bafb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "nltk.download('wordnet')                        # Lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')     # POS tagging"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CV6dH8mA0zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maps POS tag to first character lemmatize() accepts\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    dict_tag = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return dict_tag.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXmXofCZA7a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize all words with the appropriate POS tag\n",
        "def word_lemmatization(filtered_word):\n",
        "  lem_words = []\n",
        "  lem = WordNetLemmatizer()\n",
        "  lem_words = [lem.lemmatize(word, get_wordnet_pos(word)) for word in filtered_word if word]\n",
        "  return lem_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFm2B4ef16DS",
        "colab_type": "text"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCMsG_0ps0Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pip install spacy -qq "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo4q3c3uY6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities\n",
        "def ner_model(sample_conversation):\n",
        "  nlp = en_core_web_sm.load()\n",
        "  text = ' '.join(sample_conversation)\n",
        "  convo_nlp = nlp(text)\n",
        "  return convo_nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FMu-wPMat_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Renders a dependency parse tree or named entity visualization\n",
        "def ner_model_viz(document):\n",
        "  viz = displacy.render(document, jupyter=True, style='ent', page=True)\n",
        "  return viz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMMroeapaJJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts locations from NER model\n",
        "def extract_location_ner(nlp_doc):\n",
        "  gpe_list = []\n",
        "  for entity in nlp_doc.ents:\n",
        "    if entity.label_ == 'GPE':      #GPE -> Countries, cities, states\n",
        "      gpe_list.append(entity.text)\n",
        "      \n",
        "  return list(unique_everseen(gpe_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdPFg1PfjqqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74ebaf27-d4f0-4425-cb6d-48f193fee044"
      },
      "source": [
        "date_list = []\n",
        "for entity in nlp_doc.ents:\n",
        "  if entity.label_ == 'DATE':      #GPE -> Countries, cities, states\n",
        "    date_list.append(entity.text)\n",
        "    \n",
        "date_list"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['august 26th september 5th', '4 day', 'nice day']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DDWE5DSjJkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts dates from NER model \n",
        "def extract_date_ner(nlp_doc):\n",
        "  date_list = []\n",
        "  for entity in nlp_doc.ents:\n",
        "    if entity.label_ == 'DATE':      #DATE -> Absolute or relative dates or periods\n",
        "      date_list.append(entity.text)\n",
        "      \n",
        "  return date_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep8ng0TMq9YV",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZBl2u5A2VYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess frames data set using functions above \n",
        "def data_preprocess(text_data):\n",
        "  list_of_convo = text_to_convo(text_data) \n",
        "  list_of_statement = convo_to_statement(list_of_convo)\n",
        "  return list_of_statement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YRyaZ3D2bqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess a conversation using all functions above\n",
        "def convo_preprocess(conversation):\n",
        "  flat_convo = flatten_list(conversation)\n",
        "  list_of_word = word_token(flat_convo)\n",
        "  no_punct_word = punc_removal(list_of_word)\n",
        "  lower_word = word_to_lowercase(no_punct_word)\n",
        "  no_stop_word = stop_word_removal(lower_word)\n",
        "  lem_word = word_lemmatization(no_stop_word)\n",
        "  return lem_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWB3vDWsRt16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "957f2dbc-5374-4905-e54d-8ac9bd663eb6"
      },
      "source": [
        "# Preprocessing\n",
        "all_convo = data_preprocess(convo_all)\n",
        "sample_convo_train = convo_preprocess(train_convo)\n",
        "\n",
        "# Applying preprocessed text to NER\n",
        "nlp_doc = ner_model(sample_convo_train)\n",
        "ner_viz = ner_model_viz(nlp_doc)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">would like book trip \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    chicago\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " san diego \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    august 26th september 5th\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " leave \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    chicago\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " strict budget $ \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2300\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " please excuse see already told travel \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    chicago\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " adult child join yes \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    3\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " adult come well offer \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    4 day\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " stay \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    san diego\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " 211510usd rating hotel spare time good rating \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    88610\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " activity travel spring break hotel include free breakfast wifi parking unable provide detail activity shall book package yes please book complete confirmation email sent address help anything else nope everything thank thank \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    nice day\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCviR3-KeJ1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4c6ea801-d3f5-47ed-a56b-464f817d1a1e"
      },
      "source": [
        "print(extract_location_ner(nlp_doc))\n",
        "extract_date_ner(nlp_doc)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['chicago', 'san diego']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['august 26th september 5th', '4 day', 'nice day']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4W9uv_5Tsmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_convo = all_convo[114]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCL78x5OUi6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_convo = all_convo[90]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvdBuHQtKXTG",
        "colab_type": "text"
      },
      "source": [
        "# Chatterbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ12G7U4ibb7",
        "colab_type": "code",
        "outputId": "d09e7bf9-c8ff-4301-fbbf-db344f6d6c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Create a new chat bot named Charlie\n",
        "chatbot = ChatBot('Charlie')\n",
        "trainer = ListTrainer(chatbot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50vn2vPxwteJ",
        "colab_type": "code",
        "outputId": "7eb330e6-1e08-4689-d80e-f380bb1d9b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer.train([\n",
        "    'How are you?',\n",
        "    'I am good.',\n",
        "    'That is good to hear.',\n",
        "    'Thank you',\n",
        "    'You are welcome.',\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOohofQ5sgJ",
        "colab_type": "code",
        "outputId": "672bae01-3b0c-45ea-c70c-c525344f6c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer.train(flat_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzLGP8kKsBmG",
        "colab_type": "code",
        "outputId": "83ff2809-129d-4816-c043-e549b2b259f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(chatbot.get_response('Hi'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To start, just give me some information on where you would like to travel, your budget, your point of departure, or any other travel info you can think of.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVG3GUofrJ24",
        "colab_type": "code",
        "outputId": "0683069c-d081-4b26-b7ae-18f900ee6e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('I would like to travel from Toronto to Japan'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Did you have any specific dates in mind?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTYd5-j7_C1",
        "colab_type": "code",
        "outputId": "f3529a0f-593c-4a84-f376-11f69f9c625c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('June to August'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How much would that be?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9-q63wu8JRS",
        "colab_type": "code",
        "outputId": "31619179-dc76-4848-a04d-d3ae9cb11280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('under 1200'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perfect. Thank you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhAhlBR68NOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}