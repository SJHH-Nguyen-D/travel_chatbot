{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GnP_5uwHAKSP",
        "CeAAajV7zKU_",
        "yPj0T71h5Vl9",
        "L6F_mTQzFnSP",
        "DvNQtNPVG49I",
        "WpQCZqm1u8Rp",
        "EFm2B4ef16DS",
        "Ep8ng0TMq9YV",
        "FufYo5mm84KX",
        "Dm5RYCjy9zDc",
        "ywrsePGV-uEj",
        "DvdBuHQtKXTG",
        "eXfZnMsM1Y7K"
      ],
      "mount_file_id": "1OYlG4YphGkIwLI0Nnqv9HIGc-UdKa7-v",
      "authorship_tag": "ABX9TyNoAeQlgx4SQzZbEEaJjsIf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYilNPsPjxYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install chatterbot \n",
        "# ! pip install chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j15dAlF_sKtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install --upgrade chatterbot \n",
        "# ! pip install --upgrade chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnP_5uwHAKSP",
        "colab_type": "text"
      },
      "source": [
        "# Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krjAXNNE_pGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import statements\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint as pp\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import re\n",
        "from timeit import default_timer\n",
        "from  more_itertools import unique_everseen\n",
        "\n",
        "# Preprocessing\n",
        "# from pycontractions import Contractions\n",
        "\n",
        "# Tokenization imports\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Puncuation & lower case\n",
        "import string #punctuation removal\n",
        "\n",
        "# Stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Lemmatizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# POS tagging\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# NER\n",
        "import nltk, nltk.tag, nltk.chunk \n",
        "import spacy\n",
        "import pprint as pprint\n",
        "from gensim.summarization import summarize \n",
        "from collections import Counter \n",
        "import en_core_web_sm # CNN gets loaded in, sees what words depends on each other, POS tagging, entity recognition \n",
        "from spacy import displacy # Visualize NER\n",
        "\n",
        "# Chatterbot\n",
        "# from chatterbot import ChatBot\n",
        "# from chatterbot.trainers import ListTrainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAAajV7zKU_",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju63vJs-FRiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is testing with part 1 - 10% of data\n",
        "with open('/content/drive/My Drive/contraction_data_parts/expand_convo_text_1.txt', 'r') as file:\n",
        "    convo_all = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pen9_RKDsJtO",
        "colab_type": "code",
        "outputId": "8f3d78f8-1c7d-40ed-cd74-90d8deb25ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Let's see the text data, seems messy \n",
        "# When we extracted conversations from the frames dataset, we split every statement with a new line and every conversations with *\n",
        "# Through the use of pycontractions, every new line (\\n) was added with an extra \\\n",
        "pp.pprint(convo_all[0:1000]) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('[\"I would like to book a trip to Atlantis from Caprica on Saturday, August '\n",
            " '13, 2016 for 8 adults. I have a tight budget of 1700.\\\\nHi...I checked a few '\n",
            " 'options for you, and unfortunately, we do not currently have any trips that '\n",
            " 'meet this criteria.  Would you like to book an alternate travel '\n",
            " 'option?\\\\nYes, how about going to Neverland from Caprica on August 13, 2016 '\n",
            " 'for 5 adults. For this trip, my budget would be 1900.\\\\nI checked the '\n",
            " 'availability for this date and there were no trips available.  Would you '\n",
            " 'like to select some alternate dates?\\\\nI have no flexibility for dates... '\n",
            " 'but I can leave from Atlantis rather than Caprica. How about that?\\\\nI '\n",
            " 'checked the availability for that date and there were no trips available.  '\n",
            " 'Would you like to select some alternate dates?\\\\nI suppose I will speak with '\n",
            " 'my husband to see if we can choose other dates, and then I will come back to '\n",
            " 'you.Thanks for your help\\\\n******************************************Hello, '\n",
            " 'I am looking to book a vacation from Gotham Ci')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOJOPGk3Xrs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits text data into separate lists for each conversation\n",
        "def text_to_convo(text):\n",
        "  list_convos = [convo.split('\\n') for convo in text.split('*') if text]         # Conversation delimited by *\n",
        "  list_convos = [convo for convo in list_convos if convo != ['']]                # Remove empty conversation\n",
        "  return list_convos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrriLBqZoT7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits all the statements from a conversation into their own string\n",
        "def convo_to_statement(list_convos):\n",
        "  sep_convos = []\n",
        "  for items in list_convos: \n",
        "    sep_convos.append([])                     # Creates list for each conversation\n",
        "    for item in items:\n",
        "      item = item.split('\\\\n')                # Splits conversations into statements \n",
        "      sep_convos[-1].append(item)             # Adds all statements to corresponding list\n",
        "  return sep_convos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg9ULJTA7WAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flattens the list as there is a list in another list\n",
        "# - Handles sentence tokenization, as all statements are separated into their own string in this list \n",
        "def flatten_list(conversation): \n",
        "  flat_list = []\n",
        "  for sublist in conversation:\n",
        "    for item in sublist:\n",
        "          flat_list.append(item)\n",
        "  return flat_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En4lJcxsyvMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts only the users' statements from the conversations\n",
        "def extract_user_statement(conversation):\n",
        "  user_statement = conversation[0::2] \n",
        "  return user_statement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPj0T71h5Vl9",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xYbWKne5bnM",
        "colab_type": "code",
        "outputId": "f8f1e365-077a-4934-ea58-aa05cd624482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt') # Punkt sentence tokenizer"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p3Imi6DLhFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits sentences into words \n",
        "def word_token(flat_list):\n",
        "  tokenized_words=[]\n",
        "  tokenized_words.extend(word for word in word_tokenize(str(flat_list))) # Extends list by appending elements from the iterable\n",
        "  return tokenized_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6F_mTQzFnSP",
        "colab_type": "text"
      },
      "source": [
        "## Punctuation & Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDUbmbvXus2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string.punctuation = string.punctuation.replace('$','’')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7XnfJq4-mre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2a59970-74f1-4d1d-ede9-2ecbff1be4ad"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#’%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_CPUNqFTrOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# might reconsider as it removes : from time 2:03 -> 203\n",
        "# Removes punctuation from all strings \n",
        "# - Decided to use translate() to remove pesky characters that were attatched to the string\n",
        "def punc_removal(tokenized_words):\n",
        "  translator = str.maketrans('', '', string.punctuation)                        # Construct translator\n",
        "  no_punct = [word.translate(translator) for word in tokenized_words]           # Removes punctuation\n",
        "  no_punct = [word for word in no_punct if word != '']                          # Removes empty strings\n",
        "  return no_punct             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ofBua-nGuYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Makes every word lowercase \n",
        "def word_to_lowercase(word_list):\n",
        "  word_lower = [word.lower() for word in word_list]\n",
        "  return word_lower"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvNQtNPVG49I",
        "colab_type": "text"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgIaa9qiIuFP",
        "colab_type": "code",
        "outputId": "52a88aa5-71dc-42c0-f877-ae1356c64ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJUMTOcTIxZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "# add_stop_words = ['would','could']\n",
        "# stop_words += add_stop_words\n",
        "# print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6gxeqbRIfQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stop_word_removal(word_list):\n",
        "  stop_words = stopwords.words('english')\n",
        "  filtered_word = []\n",
        "  for word in word_list:\n",
        "    if word not in stop_words:\n",
        "      filtered_word.append(word)\n",
        "  return filtered_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpQCZqm1u8Rp",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatizing with appropriate POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnoQv-mzbfT",
        "colab_type": "code",
        "outputId": "2e4627b4-de2b-455f-ae31-9017f7e2cad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "nltk.download('wordnet')                        # Lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')     # POS tagging"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CV6dH8mA0zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maps POS tag to first character lemmatize() accepts\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    dict_tag = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    #print(f'{word}-----{tag}')\n",
        "    return dict_tag.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXmXofCZA7a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize all words with the appropriate POS tag\n",
        "def word_lemmatization(filtered_word):\n",
        "  lem_words = []\n",
        "  lem = WordNetLemmatizer()\n",
        "  lem_words = [lem.lemmatize(word, get_wordnet_pos(word)) for word in filtered_word if word]\n",
        "  return lem_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFm2B4ef16DS",
        "colab_type": "text"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCMsG_0ps0Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pip install spacy -qq "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo4q3c3uY6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities\n",
        "def ner_model(sample_conversation):\n",
        "  nlp = en_core_web_sm.load()\n",
        "  text = ' '.join(sample_conversation)\n",
        "  convo_nlp = nlp(text)\n",
        "  return convo_nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FMu-wPMat_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Renders a dependency parse tree or named entity visualization\n",
        "def ner_model_viz(document):\n",
        "  viz = displacy.render(document, jupyter=False, style='ent', page=True)\n",
        "  return viz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMMroeapaJJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts locations from NER model\n",
        "def extract_location_ner(nlp_doc):\n",
        "  gpe_list = []\n",
        "  for entity in nlp_doc.ents:\n",
        "    if entity.label_ == 'GPE':      #GPE -> Countries, cities, states\n",
        "      gpe_list.append(entity.text)\n",
        "      \n",
        "  return list(unique_everseen(gpe_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DDWE5DSjJkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts dates from NER model \n",
        "def extract_date_ner(nlp_doc):\n",
        "  date_list = []\n",
        "  for entity in nlp_doc.ents:\n",
        "    if entity.label_ == 'DATE':      #DATE -> Absolute or relative dates or periods\n",
        "      date_list.append(entity.text)\n",
        "      \n",
        "  return date_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noGVR936tOO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracts dates from NER model \n",
        "def extract_price_ner(nlp_doc):\n",
        "  price_list = []\n",
        "  for entity in nlp_doc.ents:\n",
        "    if entity.label_ == 'MONEY':      #Money -> Monetary values, including unit\n",
        "      price_list.append(entity.text)\n",
        "    elif entity.label == 'CARDINAL':\n",
        "      price_list.append(entity.text)\n",
        "  return price_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKlhjd571_v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ner_ouput(location_list,date_list,price_list):\n",
        "  output = {'Locations':location_list,\n",
        "            'Dates':date_list,\n",
        "            'Price':price_list}\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep8ng0TMq9YV",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZBl2u5A2VYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess frames data set using functions above \n",
        "def data_preprocess(text_data):\n",
        "  list_of_convo = text_to_convo(text_data) \n",
        "  list_of_statement = convo_to_statement(list_of_convo)\n",
        "  return list_of_statement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YRyaZ3D2bqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess a conversation using all functions above\n",
        "def convo_preprocess(conversation):\n",
        "  flat_convo = flatten_list(conversation)\n",
        "  list_user_statement = extract_user_statement(flat_convo)\n",
        "  list_of_word = word_token(list_user_statement)\n",
        "  no_punct_word = punc_removal(list_of_word)\n",
        "  lower_word = word_to_lowercase(no_punct_word)\n",
        "  no_stop_word = stop_word_removal(lower_word)\n",
        "  lem_word = word_lemmatization(no_stop_word)\n",
        "  return lem_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fql78Gic7RPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Conversations\n",
        "all_convo = data_preprocess(convo_all) # from frames dataset\n",
        "convo_1  = [['Yo what up?',\n",
        "             'Hi, how can I help you?',\n",
        "             'I wanna fly from Toronto to Tokyo',\n",
        "             'Great! What days were you looking at?',\n",
        "             'leave june 1st,back july 1st',\n",
        "             'Any price range?',\n",
        "             'I got budget $3821',\n",
        "             'Sounds good, here are some options:']]\n",
        "convo_2 = all_convo[4] # from frames dataset\n",
        "convo_3 = all_convo[10] # from frames dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FufYo5mm84KX",
        "colab_type": "text"
      },
      "source": [
        "### Sample Conversation 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWB3vDWsRt16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "771e12c4-6b00-47b3-aaa3-94106fb33b32"
      },
      "source": [
        "# Preprocessing\n",
        "sample_convo = convo_preprocess(convo_1)\n",
        "pp.pprint(convo_1)\n",
        "\n",
        "# Applying preprocessed text to NER\n",
        "nlp_doc = ner_model(sample_convo)\n",
        "ner_viz = ner_model_viz(nlp_doc)\n",
        "\n",
        "# Extracting information from NER model\n",
        "extracted_location = extract_location_ner(nlp_doc)\n",
        "extracted_date = extract_date_ner(nlp_doc)\n",
        "extracted_price = extract_price_ner(nlp_doc)\n",
        "\n",
        "ner_ouput(extracted_location, extracted_date, extracted_price)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Yo what up?',\n",
            "  'Hi, how can I help you?',\n",
            "  'I wanna fly from Toronto to Tokyo',\n",
            "  'Great! What days were you looking at?',\n",
            "  'leave june 1st,back july 1st',\n",
            "  'Any price range?',\n",
            "  'I got budget $3821',\n",
            "  'Sounds good, here are some options:']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dates': ['june 1st back july 1st'],\n",
              " 'Locations': ['toronto', 'tokyo'],\n",
              " 'Price': ['3821']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm5RYCjy9zDc",
        "colab_type": "text"
      },
      "source": [
        "### Sample Conversation 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vsmTWa09xD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "89363f70-7699-43b4-a69b-72134520232f"
      },
      "source": [
        "# Preprocessing\n",
        "sample_convo = convo_preprocess(convo_2)\n",
        "pp.pprint(convo_2)\n",
        "\n",
        "# Applying preprocessed text to NER\n",
        "nlp_doc = ner_model(sample_convo)\n",
        "ner_viz = ner_model_viz(nlp_doc)\n",
        "\n",
        "# Extracting information from NER model\n",
        "extracted_location = extract_location_ner(nlp_doc)\n",
        "extracted_date = extract_date_ner(nlp_doc)\n",
        "extracted_price = extract_price_ner(nlp_doc)\n",
        "\n",
        "ner_ouput(extracted_location, extracted_date, extracted_price)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Hello, I am looking to book a trip for 2 adults and 6 children for $21,300 '\n",
            "  'or less. We are departing from Kochi for Denver.',\n",
            "  'I have several options available within your budget. How long would you '\n",
            "  'like to travel for? And do you have dates in mind?',\n",
            "  'I do not have any dates in mind. I would like to spend as much time in '\n",
            "  'Denver as my budget will allow.',\n",
            "  'I can book 7 days at a 4.0 star hotel for 19028.93USD. I can also book 7 '\n",
            "  'days at a 3.0 star hotel for 12824.84USD.',\n",
            "  'Do these packages have different departure dates? When would I be leaving '\n",
            "  'for each of them?',\n",
            "  'The 3.0 star trip leaves Kochi August 26 and returns August 31. The 4.0 '\n",
            "  'star leaves August 27 from Kochi and returns September 1.',\n",
            "  'Ok, I would like to purchase the trip with the 4-star hotel.',\n",
            "  'Perfect, I will book that trip for you now. You will depart Kochi at 9:00 '\n",
            "  'am August 27. Have a nice day.',\n",
            "  'Thank you',\n",
            "  '']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">hello look book trip \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " adult 6 child $ \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    21300\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " less depart \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    kochi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " denver date mind would like spend much time \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    denver\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " budget allow package different departure date would leave ok would like purchase trip \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    4star\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " hotel thank</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dates': [], 'Locations': ['kochi', 'denver'], 'Price': ['21300']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywrsePGV-uEj",
        "colab_type": "text"
      },
      "source": [
        "### Sample Conversation 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVTK6Tsq-HxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "3157f22b-3882-4824-e898-4e4e2d422ed2"
      },
      "source": [
        "# Preprocessing\n",
        "sample_convo = convo_preprocess(convo_3)\n",
        "pp.pprint(convo_3)\n",
        "\n",
        "# Applying preprocessed text to NER\n",
        "nlp_doc = ner_model(sample_convo)\n",
        "ner_viz = ner_model_viz(nlp_doc)\n",
        "\n",
        "# Extracting information from NER model\n",
        "extracted_location = extract_location_ner(nlp_doc)\n",
        "extracted_date = extract_date_ner(nlp_doc)\n",
        "extracted_price = extract_price_ner(nlp_doc)\n",
        "\n",
        "ner_ouput(extracted_location, extracted_date, extracted_price)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['I would like to get away from Monday, August 15, 2016 to Wednesday, August '\n",
            "  '31, 2016. My budget is $3200 and I would leave from Detroit.',\n",
            "  'And what destination would you like to travel to?',\n",
            "  'Anywhere, I am looking for an adventure. What do you have?',\n",
            "  'How about Mexico or Porto Alegre?',\n",
            "  'let us look at both',\n",
            "  'Mexico is unavailable.  Porto Alegre is available, but you would have to '\n",
            "  'leave from Santos.',\n",
            "  'Can you please suggest some destinations that are available? I am leaving '\n",
            "  'from Detroit.',\n",
            "  'I have no departures available from Detroit. for these dates.  Would you '\n",
            "  'like to make a date change?',\n",
            "  'Anything between August 15 and August 31?',\n",
            "  'I am sorry, but we do not have any departures from Detroit available for '\n",
            "  'these dates for the budget you provided.  Would you like to adjust your '\n",
            "  'budget for this trip?',\n",
            "  'No I cannot. Thank you for your help',\n",
            "  '']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">would like get away \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    monday august 15 2016\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    wednesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    august 31\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " 2016 budget $ \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    3200\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " would leave \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    detroit\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " anywhere look adventure let u look please suggest destination available leave \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    detroit\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " anything \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    august 15 august 31\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " thank help</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dates': ['monday august 15 2016',\n",
              "  'wednesday',\n",
              "  'august 31',\n",
              "  'august 15 august 31'],\n",
              " 'Locations': ['detroit'],\n",
              " 'Price': ['3200']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvdBuHQtKXTG",
        "colab_type": "text"
      },
      "source": [
        "# Chatterbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXfZnMsM1Y7K",
        "colab_type": "text"
      },
      "source": [
        "## Next Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ12G7U4ibb7",
        "colab_type": "code",
        "outputId": "d09e7bf9-c8ff-4301-fbbf-db344f6d6c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Create a new chat bot named Charlie\n",
        "chatbot = ChatBot('Charlie')\n",
        "trainer = ListTrainer(chatbot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50vn2vPxwteJ",
        "colab_type": "code",
        "outputId": "7eb330e6-1e08-4689-d80e-f380bb1d9b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer.train([\n",
        "    'How are you?',\n",
        "    'I am good.',\n",
        "    'That is good to hear.',\n",
        "    'Thank you',\n",
        "    'You are welcome.',\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOohofQ5sgJ",
        "colab_type": "code",
        "outputId": "672bae01-3b0c-45ea-c70c-c525344f6c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer.train(flat_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzLGP8kKsBmG",
        "colab_type": "code",
        "outputId": "83ff2809-129d-4816-c043-e549b2b259f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(chatbot.get_response('Hi'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To start, just give me some information on where you would like to travel, your budget, your point of departure, or any other travel info you can think of.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVG3GUofrJ24",
        "colab_type": "code",
        "outputId": "0683069c-d081-4b26-b7ae-18f900ee6e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('I would like to travel from Toronto to Japan'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Did you have any specific dates in mind?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTYd5-j7_C1",
        "colab_type": "code",
        "outputId": "f3529a0f-593c-4a84-f376-11f69f9c625c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('June to August'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How much would that be?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9-q63wu8JRS",
        "colab_type": "code",
        "outputId": "31619179-dc76-4848-a04d-d3ae9cb11280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.get_response('under 1200'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perfect. Thank you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhAhlBR68NOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}